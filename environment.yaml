# Conda environment for PySpark

# Creating an environment from an environment.yml file
# https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file

name: pyspark_env
channels:
  - defaults
  - conda-forge
dependencies:
  # Python 3.9 is not yet supported (April 2021) due to PyArrow dependencies
  # https://spark.apache.org/docs/latest/index.html#downloading
  - python=3.8
  - pyspark=3.1
